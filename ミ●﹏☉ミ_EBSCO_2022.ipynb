{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b87983d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp\\TEMP\\ipykernel_1260\\3822443864.py:30: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  self.browser = webdriver.Chrome(self.driver_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在爬取EBSCO\n",
      "共 301 筆\n",
      "有 7 頁\n",
      "最後一頁剩 1 筆\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp\\TEMP\\ipykernel_1260\\3822443864.py:88: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  res_process = res_process.append(result_df)\n",
      "C:\\Temp\\TEMP\\ipykernel_1260\\3822443864.py:88: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  res_process = res_process.append(result_df)\n",
      "C:\\Temp\\TEMP\\ipykernel_1260\\3822443864.py:96: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  res_all = res_all.append(res_process)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中途存檔成功\n",
      "成功爬取EBSCO - 台灣社會變遷基本調查(TSCS)\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import csv\n",
    "\n",
    "\n",
    "class EBSCO_CRAWLER():\n",
    "\n",
    "    def __init__(self, driver_path, SaveFile_Path):\n",
    "        self.url_to_crawl = 'http://search.ebscohost.com/login.asp?profile=ehost&defaultdb=lxh'\n",
    "        self.driver_path = driver_path\n",
    "        self.SaveFile_Path = SaveFile_Path\n",
    "        self.keys = keys\n",
    "        self.gg = gg\n",
    "\n",
    "    def start(self):\n",
    "        self.browser = webdriver.Chrome(self.driver_path)\n",
    "        self.browser.maximize_window()\n",
    "\n",
    "    def get_into_page(self, url):\n",
    "        self.browser.get(url)\n",
    "        time.sleep(5)\n",
    "        #先點一次查詢，進到搜尋主頁面\n",
    "        self.browser.find_element(By.ID, 'selectDBLink').click()        \n",
    "        time.sleep(5)\n",
    "        self.browser.find_element(By.ID, 'selectAll').click()        \n",
    "        self.browser.find_element(By.ID, 'btnOKTop').click()        \n",
    "        time.sleep(5)\n",
    "\n",
    "    def search_work(self):\n",
    "        self.browser.find_element(By.ID, \"Searchbox1\").send_keys(keys)\n",
    "        self.browser.find_element(By.ID, \"SearchButton\").click()\n",
    "        soup = BeautifulSoup(self.browser.page_source, 'html.parser')\n",
    "        page_str = soup.find('h1', {'class': 'page-title alt'})\n",
    "        page_text = page_str.text\n",
    "        pattern = '[0-9]+'\n",
    "        page_re = re.findall(pattern, page_text)[2]\n",
    "        page_int = int(page_re)#筆數\n",
    "        page_last = page_int % 50\n",
    "        if page_int % int(50) == 0:\n",
    "            self.page_all = page_int // int(50)\n",
    "        else:\n",
    "            self.page_all = (page_int // int(50)) + 1  #頁數\n",
    "        print('共 ' + str(page_int) + ' 筆')\n",
    "        print('有 ' + str(self.page_all) + ' 頁')\n",
    "        print('最後一頁剩 ' + str(page_last) + ' 筆')\n",
    "\n",
    "    def download_file(self, soup):\n",
    "        result_list = []\n",
    "        ttl_list = soup.find_all('a', {'class': 'title-link color-p4'})\n",
    "        content_list = soup.find_all('div', {'class': 'display-info'})\n",
    "        for row in range(0, len(ttl_list)):\n",
    "            title = ttl_list[row].text  #標題\n",
    "            content = content_list[row].text  #作者、期刊名稱、頁數、DOI\n",
    "            remove = content[content.find('Add to folder'):len(content)]\n",
    "            content = ' '.join(\n",
    "                content.replace(remove, \"\").replace(\"\\n\", '').split())\n",
    "            db = content[content.find('Database: '):len(content)]\n",
    "            info = content[0:content.find(', Database: ')]\n",
    "            result_list.append([(self.page - 1) * 50 + row + 1, title,\n",
    "                                content, info,db])\n",
    "        return result_list\n",
    "\n",
    "    def roll_page(self):  #翻頁並進入search_wor和download_file的步驟\n",
    "        self.page = 1\n",
    "        i = 1\n",
    "        res_process = []\n",
    "        res_all = pd.DataFrame()\n",
    "        while self.page <= self.page_all:\n",
    "            soup = BeautifulSoup(self.browser.page_source, 'html.parser')\n",
    "            result_list = self.download_file(soup)\n",
    "            time.sleep(5)\n",
    "            result_df = pd.DataFrame(result_list)\n",
    "            res_process = pd.DataFrame(res_process)\n",
    "            res_process = res_process.append(result_df)\n",
    "            self.page = self.page + 1\n",
    "            try:\n",
    "                self.browser.find_element(By.ID,\n",
    "                    \"ctl00_ctl00_MainContentArea_MainContentArea_bottomMultiPage_lnkNext\"\n",
    "                ).click()\n",
    "                time.sleep(random.randint(5, 25))  #避開伺服器察覺相同頻率爬取資料  ✔️✔️✔️重要\n",
    "            except:\n",
    "                res_all = res_all.append(res_process)\n",
    "                res_all.columns = [\"筆數\",\"原始題目\",\"資訊\",\"資訊2\",\"資料庫\"]\n",
    "                try:\n",
    "                    res_all.to_csv(SaveFile_Path + '\\\\' + SaveFile_Name + \"_\" +\n",
    "                                   \"_\" + time.strftime(\"%m%d\") + \"_\" +\n",
    "                                   time.strftime(\"%H%M\") + \".csv\",\n",
    "                                   encoding=\"utf_8_sig\",\n",
    "                                   index=False)  #儲存結果\n",
    "                    print(\"中途存檔成功\")  \n",
    "                    break\n",
    "                except:\n",
    "                    print(\"存檔失敗，請檢察結果檔是否正在開啟，或處於無法被寫入的狀態。\")\n",
    "            if self.page == self.page_all:\n",
    "                print(\"此步驟為爬取最終頁資料\")\n",
    "                soup = BeautifulSoup(self.browser.page_source, 'lxml')\n",
    "                result_list = self.download_file(soup)\n",
    "                time.sleep(5)\n",
    "                result_df = pd.DataFrame(result_list)\n",
    "                res_process = pd.DataFrame(res_process)\n",
    "                res_process = res_process.append(result_df)\n",
    "                res_all = res_all.append(res_process)\n",
    "                res_all.columns = [\"筆數\",\"原始題目\",\"資訊\",\"資訊2\",\"資料庫\"]\n",
    "                try:\n",
    "                    res_all.to_excel(SaveFile_Path + '\\\\' + SaveFile_Name + \"_\" +\n",
    "                                   \"_\" + time.strftime(\"%m%d\") + \"_\" +\n",
    "                                   time.strftime(\"%H%M\") + \".xlsx\",\n",
    "                                   encoding=\"utf_8_sig\",\n",
    "                                   index=False)  #儲存結果\n",
    "                    print(\"存檔成功\")  #最終檔名會加上 1.月份 2.日期 3.時間(時分)\n",
    "                    break\n",
    "                except:\n",
    "                    print(\"存檔失敗，請檢察結果檔是否正在開啟，或處於無法被寫入的狀態。\")\n",
    "\n",
    "    def close_driver(self):\n",
    "        self.browser.quit()\n",
    "        print('成功爬取EBSCO - ' + Dataset)\n",
    "\n",
    "    def step(self):\n",
    "        self.start()\n",
    "        print('正在爬取EBSCO')\n",
    "        self.get_into_page(self.url_to_crawl)\n",
    "        self.search_work()\n",
    "        self.roll_page()\n",
    "        self.close_driver()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    driver_path = r'C:\\Users\\user\\Desktop\\py\\Driver\\chromedriver.exe'  #chromedriver位置\n",
    "    SaveFile_Path = r'C:\\Users\\user\\Desktop\\py\\衍伸書目\\EBSCO'  #要儲存檔案的路徑\n",
    "    keys_path = r'C:\\Users\\user\\Desktop\\py\\指令整理.xlsx'  #指令\n",
    "    gg = 1\n",
    "    ALLkeys = pd.read_excel(keys_path)\n",
    "    ALLkeys = ALLkeys[120:144]\n",
    "    keys = ALLkeys['Final'].iloc[gg - 1]\n",
    "    Dataset = ALLkeys['Dataset'].iloc[gg - 1]\n",
    "    SaveFile_Name = r'EBSCO' + Dataset\n",
    "    go = EBSCO_CRAWLER(driver_path, SaveFile_Path)\n",
    "    go.step()\n",
    "\n",
    "# 1. 台灣社會變遷基本調查(TSCS)\n",
    "# 2. 華人家庭動態資料庫(PSFD)\n",
    "# 3. 台灣教育長期追蹤(TEPS)\n",
    "# 4. 特殊教育長期追蹤資料庫(SNELS)\n",
    "# 5. 家庭收支調查(SFIE)\n",
    "# 6. 台灣世界價值觀調查(TWVS)\n",
    "# 7. 台灣青少年成長歷程研究(TYP)\n",
    "# 8. 台灣選舉調查資料(TEDS)\n",
    "# 9. 「台灣教育長期追蹤資料庫」後續調查(TEPS-B)\n",
    "# 10. 人力資源調查(MPS)\n",
    "# 11. 人力運用調查(MUS)\n",
    "# 12. 婦女婚育與就業調查(WMFES)\n",
    "# 13. 個人家戶數位機會(落差)(數位落差)(DDS)\n",
    "# 14. 台灣貧窮兒少資料庫(TDCYP)\n",
    "# 15. 台灣傳播調查資料庫(TCS)\n",
    "# 16. International Social Survey Programme (ISSP)\n",
    "# 17. 幼兒發展調查資料庫建置計畫\n",
    "# 18. 國人旅遊狀況調查\n",
    "# 19. 來臺旅客消費及動向調查\n",
    "# 20. 台灣社會意向調查\n",
    "# 21. 台灣營養健康狀況變遷調查\n",
    "# 22. 我國大學生政治社會化的定群追蹤研究\n",
    "# 23. 台灣政府文官調查\n",
    "# 24. 青少年藥物濫用之起因"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442aaef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671c2805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2598e70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520e4274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd90e90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a2e805",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
